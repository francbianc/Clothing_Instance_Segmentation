{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion: VGG Image Annotator format to COCO \n",
    "Necessary actions to modify the VIA json file before feeding it into the converting function in ```converters.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001\n"
     ]
    }
   ],
   "source": [
    "# Step: REMOVE DATA - Remove no-annotated images\n",
    "path_with_notannos = '/Users/francescabianchessi/THESIS/ANNOTATING_DF2/df_shop.json'\n",
    "js = json.load(open(path_with_notannos))\n",
    "new_js = js.copy()\n",
    "new_js['_via_img_metadata'] = {k:v for k,v in js['_via_img_metadata'].items() if int(k) <= 3083 and v['regions']}\n",
    "new_js['_via_image_id_list'] = [name for name in js['_via_image_id_list'] if str(name) in new_js['_via_img_metadata'].keys()]\n",
    "print(len(new_js['_via_image_id_list']))\n",
    "\n",
    "with open('post_delete.json', 'w') as f:\n",
    "    json.dump(new_js, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1893 500 3.786\n"
     ]
    }
   ],
   "source": [
    "# Step: BASE - Open a json and checks \n",
    "path_annos_train_val = '/Users/francescabianchessi/THESIS/ANNOTATING_DF2/PROVOOO.json'\n",
    "j = json.load(open(path_annos_train_val)) #'_via_img_metadata', '_via_attributes', '_via_data_format_version', '_via_image_id_list'\n",
    "assert list(j['_via_img_metadata'].keys()) == [str(i) for i in j['_via_image_id_list']]\n",
    "\n",
    "for d in j['_via_img_metadata'].values(): # check all images have annos\n",
    "    for r in d['regions']:\n",
    "        try:\n",
    "            assert r['region_attributes']['clothing']\n",
    "        except: \n",
    "            print(d['filename']) # names images without annos\n",
    "\n",
    "masks_count = []\n",
    "for k, v in j['_via_img_metadata'].items():\n",
    "    masks_count.append((int(k.split('.jpg')[0]), len(j['_via_img_metadata'][k]['regions'])))\n",
    "\n",
    "ids_segmented = []\n",
    "for t in masks_count:\n",
    "    if t[1] > 0:\n",
    "        ids_segmented.append(t[0])\n",
    "\n",
    "assert len(ids_segmented) == len(j['_via_img_metadata'])\n",
    "num_masks = sum([t[1] for t in masks_count])\n",
    "num_images = len(j['_via_img_metadata'])\n",
    "\n",
    "print(num_masks, num_images, num_masks/num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 501\n",
      "2500\n",
      "501\n"
     ]
    }
   ],
   "source": [
    "# Step: SPLIT TRAIN/VALIDATION\n",
    "train, val = {}, {}\n",
    "for i, k in enumerate(j['_via_img_metadata']):\n",
    "    if i < 2500: # customize here @@@\n",
    "        train[k] = j['_via_img_metadata'][k]\n",
    "    else:\n",
    "        val[k] = j['_via_img_metadata'][k]\n",
    "print(len(train), len(val))\n",
    "\n",
    "# TRAIN\n",
    "new_js_tr = j.copy()\n",
    "new_js_tr['_via_img_metadata'] = train\n",
    "new_js_tr['_via_image_id_list'] = [name for name in j['_via_image_id_list'] if str(name) in train.keys()]\n",
    "print(len(new_js_tr['_via_image_id_list']))\n",
    "\n",
    "with open('train_'+str(len(train))+'.json', 'w') as f:\n",
    "    json.dump(new_js_tr, f)\n",
    "\n",
    "# VAL\n",
    "new_js_v = j.copy()\n",
    "new_js_v['_via_img_metadata'] = val\n",
    "new_js_v['_via_image_id_list'] = [name for name in j['_via_image_id_list'] if str(name) in val.keys()]\n",
    "print(len(new_js_v['_via_image_id_list']))\n",
    "\n",
    "with open('val_'+str(len(val))+'.json', 'w') as f:\n",
    "    json.dump(new_js_v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: GET FILENAMES - step necessary to rename images and save them in a new folder\n",
    "# NB: need to run BASE step\n",
    "filenames = [v['filename'] for v in j['_via_img_metadata'].values()]\n",
    "assert len(filenames) == len(ids_segmented) == len(j['_via_img_metadata'].keys())\n",
    "\n",
    "with open('names_images__.txt', 'w') as f:\n",
    "    f.write('\\n'.join(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: RENAME + MOVE NEW FOLDER - Run copy_files.py + rename in json\n",
    "path_new_folder = '/Users/francescabianchessi/THESIS/CODE_TESI/datasets/dlcv/dlcv_test'\n",
    "check_copy = os.listdir(path_new_folder)\n",
    "assert len(filenames) == len(ids_segmented) == len(j['_via_img_metadata'].keys()) == len(check_copy)\n",
    "\n",
    "i = 0\n",
    "for k,v in j['_via_img_metadata'].items():\n",
    "    v['filename'] = str(i).zfill(6)+'.jpg'\n",
    "    i+=1\n",
    "\n",
    "with open('via_df2_ts_'+str(len(check_copy))+'.json', 'w') as f:\n",
    "    json.dump(j, f)\n",
    "\n",
    "# Open with VIA to check: re-save with custom project name"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
